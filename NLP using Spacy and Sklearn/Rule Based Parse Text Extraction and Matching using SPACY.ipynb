{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TO KNOW MORE ABUOUT SPACY\n",
    " \n",
    " CHECK HERE : https://spacy.io/usage/rule-based-matching#matcher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "    Matching extraction will work independent of SPACES in paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\":\"iPhone\"}, {\"ORTH\":\"X\"}]  #here each dictionary is a 1 token..\n",
    "#ORTH' - IS EITHER UPPER CASE OR LOWER CASE.\n",
    "#ORTH--> matching word should be match with exact given word..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('IPHONE_PATTERN', None ,pattern)  #'some_name', callbakcs = None, pattern to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"New iPhone X release date leaked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matcher(doc1) --> returuns a list of tuples [(),()]..\n",
    "\n",
    "each tuple consists of - match_id, start_index of matched span , end_index of matched span\n",
    "(match_id - hash value of pattern name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google I/O\n"
     ]
    }
   ],
   "source": [
    "#printing matched words..\n",
    "for match_id, start , end in matches:\n",
    "    matched_span = doc1[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LEMMA':'love','POS':'VERB'}, {'POS':'NOUN'}, {'POS':'ADV', 'OP':'+'}] \n",
    "#here we are matching verb form love along with noun word\n",
    "#try to keep \"*\" in place of \"+\" and see the changes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I loved dogs but now I love cats more rapidly\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('LOVED_ONE', None ,pattern1)  #callbakcs = None and 1st argument should be something string.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11819021744060373232, 6, 9), (11819021744060373232, 6, 10)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love cats more\n",
      "love cats more rapidly\n"
     ]
    }
   ],
   "source": [
    "#printing matched words..\n",
    "for match_id, start , end in matches:\n",
    "    matched_span = doc1[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with REGULAR EXPRESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my phone number is 1256. oh it's wrong. correct one is 1256789987. ok call me!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x0000026A0A77F6A0>\n",
      "Phone number =  1256789987\n"
     ]
    }
   ],
   "source": [
    "phno = re.finditer('[0-9]{10}', text)   #it returns a string which has 10 digit number..\n",
    "print(phno)\n",
    "for i in phno:\n",
    "    print('Phone number = ',i.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wildcard text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ph']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'p.', text)  #returns a list of words that starts with 'p' and make sure we place '....' 4 dots since len('phone')=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phone']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'p....', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\W$', text)  #returns a list containing that special character, if our text is ended with any special character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extracing similar strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my phone number is ', \". oh it's wrong. correct one is \", '. ok call me!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\d]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1256', '1256789987']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[^\\D]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"you can live-long without me on KGP-Talkies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live-long', 'KGP-Talkies']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+-[\\w]+\", text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALL BACK METHOD IN Matcher.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Google annonced a new Pixel at Google I/O . Google I/O is great place to get all updtes from Google.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the above text, should have a space between \"Google I/O\" and \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'TEXT':'Google'}, {'TEXT':'I'}, {'TEXT':'/'}, {'TEXT':'O'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_method(matcher, doc, i, matches): \n",
    "    print(i)        #prints the starting index of matches.\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = doc[start:end]\n",
    "    print(entity.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### callback_method()\n",
    "this method executes when we run the \"matcher(doc)\" then pointer returns to callback_method() as inputs -\n",
    " 1)matcher object(pattern), 2)doc string, 3)firstly \"i\" pointing to zeroth location of matched tuples, 4)list of matched tuples.\n",
    "##### NOTE:\n",
    "     callback_method() executed by Matcher object automatically call until indexes(returned) are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Google', callback_method, pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Google I/O\n",
      "1\n",
      "Google I/O\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11578853341595296054, 6, 10), (11578853341595296054, 11, 15)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can also do this task with regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google I/O', 'Google I/O']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\"Google I/O\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text extration using Lingustic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say if you are analysing user comments and you want to find out what people are saying about Facebook.\n",
    "You  want to start by finding sentences containing following as'Facebook is' and 'Facebook was'. so. lets begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\":\"facebook\"}, {\"LEMMA\":\"be\"}, {\"POS\":\"ADV\",\"OP\":\"*\"}, {\"POS\": \"ADJ\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_method_fb(matcher, doc, i, matches):\n",
    "    matched_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    sent = span.sent    #it returns a  corresponding sentence in which \"span\" is present\n",
    "    \n",
    "    \n",
    "    match_ents = [{\n",
    "        \n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        'end': span.end_char - sent.start_char,\n",
    "        'label': 'MATCH'\n",
    "    }]\n",
    "    \n",
    "    matched_sents.append({'text': sent.text, 'ents': match_ents})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"fb\", callback_method_fb, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I'd say that Facebook is evil. - Facebook is pretty cool, right?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8017838677478259815, 4, 7), (8017838677478259815, 9, 13)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'd say that Facebook is evil.\",\n",
       "  'ents': [{'start': 13, 'end': 29, 'label': 'MATCH'}]},\n",
       " {'text': 'Facebook is pretty cool, right?',\n",
       "  'ents': [{'start': 0, 'end': 23, 'label': 'MATCH'}]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I'd say that \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Facebook is evil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ".</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Facebook is pretty cool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ", right?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(matched_sents, style = 'ent', manual = True)\n",
    "#manual (True): it tells us that Don't parse `Doc` instead we expect a dict/list of dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone numbers extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phone numbers will have different format in different countries. so your matched pattern will have to look for number sequences \n",
    "satistys required condition.. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "E.g : ph.no: (123) 4567 8901 or (123) 4567-8901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"dddd\"}, {\"ORTH\": \"-\", \"OP\":\"?\"}, {\"SHAPE\": \"dddd\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PhoneNumber\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Call me at (123) 4560 7890\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'at', '(', '123', ')', '4560', '7890']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7978097794922043545, 3, 8)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123) 4560 7890\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Address matching using REGEX in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"[a-z0-9-_.]+@[a-z0-9.]+\"}}]  #here \"+\" sign indicates to add all characters matched.\n",
    "#NOTE: there shouldn't be any space between {\"REGEX\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Email\", None, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Email me at kartheeknella2@gmail.com and nellakartheek2000@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Email', 'me', 'at', 'kartheeknella2@gmail.com', 'and', 'nellakartheek2000@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11010771136823990775, 3, 4), (11010771136823990775, 5, 6)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kartheeknella2@gmail.com\n",
      "nellakartheek2000@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for match_id, start,end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags and emoji detection in social media"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "by default spacy tokenizer will split emoji into seperate tokens."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vaild hashtags usually consist of \"#\" plus a secquence of ASCII characters with \"no whitespaces\", making them easy to match well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emoji = [\"😃\",\"😋\",\"🥰\",\"😘\",\"😃\",\"🤗\"] \n",
    "neg_emoji = [\"😣\",\"😔\",\"☹️\",\"😬\",\"😩\",\"😡\"]  #make sure that no space is given between \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'ORTH': '😃'}],\n",
       " [{'ORTH': '😋'}],\n",
       " [{'ORTH': '🥰'}],\n",
       " [{'ORTH': '😘'}],\n",
       " [{'ORTH': '😃'}],\n",
       " [{'ORTH': '🤗'}]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":\n",
    "        doc.sentiment += 0.1       #initially doc.sentiment = 0.0 .\n",
    "    elif doc.vab.string[match_id] == 'SAD':\n",
    "        doc.sentiment -= 0.1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"HAPPY\", label_sentiment, *pos_patterns)  #if we give a (more than one pattern)list of patterns,\n",
    "                                                              #then syntax is -- *Patterens\n",
    "matcher.add(\"SAD\", label_sentiment, *neg_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"HASHTAG\", None, [{\"TEXT\":\"#\"}, {\"IS_ASCII\": True}]) #'IS_ASCII' -- is used to receive a name with no white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello World 😃 #NELLAKartheek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '😃', '#', 'NELLAKartheek']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2686646543460454932, 2, 3), (16536914698459818706, 3, 5)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "        if spacy having in-built objects or functions then those can be used directly outside the callback method by without              returning those objects or functions in call back method.\n",
    "           below line is example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "print(doc.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPY      😃\n",
      "HASHTAG    #NELLAKartheek\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(f'{string_id:10}', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient phrase matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if you need to match large terminology lists,then you can also use the PhraseMatcher and creates Doc objects instead of token patterns, which is much more efficient overall. The Doc patterns can contain single or multile tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher= PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['BARAC OBAMA', 'ANGELA MERKEL', 'WASHINGTON D.C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BARAC OBAMA, ANGELA MERKEL, WASHINGTON D.C]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = [nlp.make_doc(text) for text in terms]   #make_doc -- it makes text string into doc\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('term', None, *pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"German Chancellor ANGELA MERKEL and US Predident BARAC OBAMA \"\n",
    "         \"converse in the Oval Office inside the white House in WASHINGTON D.C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['German', 'Chancellor', 'ANGELA', 'MERKEL', 'and', 'US', 'Predident', 'BARAC', 'OBAMA', 'converse', 'in', 'the', 'Oval', 'Office', 'inside', 'the', 'white', 'House', 'in', 'WASHINGTON', 'D.C']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "German Chancellor ANGELA MERKEL and US Predident BARAC OBAMA converse in the Oval Office inside the white House in WASHINGTON D.C"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4519742297340331040, 2, 4),\n",
       " (4519742297340331040, 7, 9),\n",
       " (4519742297340331040, 19, 21)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANGELA MERKEL\n",
      "BARAC OBAMA\n",
      "WASHINGTON D.C\n"
     ]
    }
   ],
   "source": [
    "for match_id, start,end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Rule Based Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to add named entities based on pattern dictionaries and makes it easy to combine rule-based and statistical named entity recognition for even more powerful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Patterns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In our example \"KGPTalkie is opening its first big office in San Francisco.\" may our model doesn't recognise 'KGPTalkie' as\n",
    "'ORG' and 'San Francisco' as 'GPE' but it recognises \"first\" as 'ORDINAL'.So to make changes we use rule based entity recognitn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The entity ruler accepts 2 types of patterns:\n",
    "        1) Phrase Pattern = {\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "        2) Token Pattern = {\"label\": \"GPE\", \"pattern\":[{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working of entity ruler"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "entity ruler is a pipeline component so we add via nlp.add_pipe. when nlp object is called on a text, it will find matches in the doc and add them as entities to the doc.ents, using the specified pattern label as entity label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK HERE : https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = EntityRuler(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [{\"label\":\"ORG\",\"pattern\": \"KGPTalkie\"}, {\"label\": \"GPE\", \"pattern\":[{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"KGPTalkie is opening its first big office in San Francisco.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KGPTalkie', 'is', 'opening', 'its', 'first', 'big', 'office', 'in', 'San', 'Francisco', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGPTalkie            ORG\n",
      "first                ORDINAL\n",
      "San Francisco        GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'{ent.text:20}', ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "  TO GET the explanation about terms in named entity regonition, then use \"spacy.explain(\"ORG\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
