{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "    we can can't disable tokenizer instaead we can disable tagger, dep_parser, named entity recognizer etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') #for normal processing like named entity recognizer, tokenisation etc \"sm\" is better..\n",
    "#but for word to vector etc we need large model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('This is raw text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "    WHEN we are working with large text or data then working process is little bit slow. So it is more efficient when it divided into batches of texts.\n",
    "    nlp.pipe() ---> takes an iterable of texts and yields doc objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"This is raw text\", \"There is lots of text you know\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is raw text, There is lots of text you know]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(nlp.pipe(texts))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling unnecessacy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use my number to take phone calls, take care of fake calls\n",
      "not writning anuthing from there.\n",
      "Net billion was working at office not coming to home umtil finishing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = [\"Please use my number to take phone calls, take care of fake calls\" ,\n",
    "         \"not writning anuthing from there.\",\n",
    "         \"Net billion was working at office not coming to home umtil finishing\"]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')   #spacy.load('en_core_web_sm', disable = ['tagger', 'parser'])\n",
    "\n",
    "for doc in nlp.pipe(texts, disable = ['tagger', 'parser']):\n",
    "    print(doc)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling and Restrore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. by context manager.\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    doc = nlp('i wont to be docator')\n",
    "    doc = nlp('I wil be tagee a single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. by restroing manually.\n",
    "disabled = nlp.disable_pipes('ner')\n",
    "doc = nlp(\"I won't be able to use ents\")\n",
    "disabled.restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### another way of using nlp.pipe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "text = \"Please use my number to take phone calls, take care of fake calls\" ,\"not writning anuthing from there.\",\"Net billion was working at office not coming to home umtil finishing\"\n",
    "for texts in nlp.pipe(text):\n",
    "    for p in texts:\n",
    "        if p.text == 'from':\n",
    "            print(p.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working of nlp.pipe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nlp.pipe() -- returns a generator that yields Doc objects â€“ not a list, which is better for large volumes of text.\n",
    "              index always starts with 0 with each iterator of doc object. It must be in the format of -- \" \",\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
